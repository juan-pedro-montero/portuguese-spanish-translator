{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "08c52d94",
   "metadata": {},
   "source": [
    "# NOTEBOOK PARTE 2: MODELADO TRADUCTOR PT -> ES"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa342d0b",
   "metadata": {},
   "source": [
    "# CONFIGURACIÓN DEL ENTORNO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "111ff64c",
   "metadata": {},
   "source": [
    "## Librerías"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "529050a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import torch\n",
    "from tqdm.auto import tqdm\n",
    "from datasets import Dataset  \n",
    "from datasets import DatasetDict \n",
    "from datasets import load_dataset\n",
    "from transformers import set_seed   \n",
    "from transformers import AutoTokenizer              "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41fd8ced",
   "metadata": {},
   "source": [
    "## Reproducibilidad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "42111e82",
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 333\n",
    "set_seed(SEED)\n",
    "random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f408fc24",
   "metadata": {},
   "source": [
    "## Selección dispositivo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ca4eac2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Usando dispositivo: cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Usando dispositivo: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be799c3a",
   "metadata": {},
   "source": [
    "## Path de los datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "011bd31c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archivos disponibles:\n",
      "   • ./data\\clean_corpus_pt.txt\n",
      "   • ./data\\clean_corpus_es.txt\n"
     ]
    }
   ],
   "source": [
    "DATA_DIR = \"./data\"\n",
    "PT_FILE  = os.path.join(DATA_DIR, \"clean_corpus_pt.txt\")\n",
    "ES_FILE  = os.path.join(DATA_DIR, \"clean_corpus_es.txt\")\n",
    "\n",
    "assert os.path.isfile(PT_FILE), f\"No encontrado: {PT_FILE}\"\n",
    "assert os.path.isfile(ES_FILE), f\"No encontrado: {ES_FILE}\"\n",
    "print(f\"Archivos disponibles:\\n   • {PT_FILE}\\n   • {ES_FILE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6c41296",
   "metadata": {},
   "source": [
    "# Carga del corpus preprocesado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b74deb20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pares cargados: 2480081\n",
      "Columnas disponibles: ['pt', 'es']\n"
     ]
    }
   ],
   "source": [
    "# Carga del corpus limpio desde JSON\n",
    "# Ruta al archivo JSON generado en Notebook 1\n",
    "JSON_FILE = os.path.join(DATA_DIR, \"clean_corpus_pt_es.json\")\n",
    "\n",
    "# Cargar el Dataset de Hugging Face directamente desde JSON\n",
    "corpus_dataset = load_dataset(\n",
    "    \"json\",\n",
    "    data_files=JSON_FILE,\n",
    "    field=None,          # cada objeto JSON ya es {\"pt\": ..., \"es\": ...}\n",
    "    split=\"train\"        # se carga todo como un único split \"train\"\n",
    ")\n",
    "\n",
    "# Verificar número de ejemplos y columnas\n",
    "print(f\"Pares cargados: {len(corpus_dataset)}\")\n",
    "print(\"Columnas disponibles:\", corpus_dataset.column_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a9d20d3",
   "metadata": {},
   "source": [
    "# División del dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "61e24163",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entrenamiento: 1984064 ejemplos\n",
      "     Prueba : 496017 ejemplos\n"
     ]
    }
   ],
   "source": [
    "# Definir proporciones\n",
    "TEST_SIZE = 0.2  # 20% para test\n",
    "\n",
    "# Usar el método incorporado de HF Datasets\n",
    "splits = corpus_dataset.train_test_split(test_size=TEST_SIZE, seed=SEED)\n",
    "\n",
    "# Renombrar las particiones\n",
    "dataset_dict = DatasetDict({\n",
    "    \"train\": splits[\"train\"],\n",
    "    \"test\" : splits[\"test\"]\n",
    "})\n",
    "\n",
    "# Comprobar tamaños\n",
    "print(f\"Entrenamiento: {len(dataset_dict['train'])} ejemplos\")\n",
    "print(f\"     Prueba : {len(dataset_dict['test'])} ejemplos\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32ec210b",
   "metadata": {},
   "source": [
    "# Tokenización y preparación de los datos de entrada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d9494c1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carga del tokenizer M2M100\n",
    "MODEL_NAME = \"facebook/m2m100_418M\"\n",
    "tokenizer  = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "# Configurar idiomas de origen y destino\n",
    "tokenizer.src_lang  = \"pt\"\n",
    "tokenizer.tgt_lang  = \"es\"\n",
    "forced_bos_token_id = tokenizer.get_lang_id(\"es\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "59ec22a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Función de tokenización \n",
    "MAX_LENGTH = 128\n",
    "\n",
    "def tokenize_fn(batch):\n",
    "    # Tokenizar simultáneamente entrada y etiqueta\n",
    "    outputs = tokenizer(\n",
    "        batch[\"pt\"],\n",
    "        text_target=batch[\"es\"],\n",
    "        max_length=MAX_LENGTH,\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        return_attention_mask=True\n",
    "    )\n",
    "    # outputs ya contiene 'input_ids', 'attention_mask' y 'labels' correctos,\n",
    "    # donde 'labels' tiene padding token id (no -100 aún).\n",
    "    # Convertimos padding en -100\n",
    "    labels = [\n",
    "        [(token_id if token_id != tokenizer.pad_token_id else -100)\n",
    "         for token_id in seq]\n",
    "        for seq in outputs[\"labels\"]\n",
    "    ]\n",
    "    outputs[\"labels\"] = labels\n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "15d5f5a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columnas tras tokenización: ['input_ids', 'attention_mask', 'labels']\n",
      "Ejemplos train: 1984064\n",
      "Ejemplos test : 496017\n"
     ]
    }
   ],
   "source": [
    "# Aplicar tokenización\n",
    "tokenized_datasets = dataset_dict.map(\n",
    "    tokenize_fn,\n",
    "    batched=True,\n",
    "    remove_columns=[\"pt\", \"es\"],\n",
    "    desc=\"Tokenizando con M2M100 (text_target)\",\n",
    "    load_from_cache_file=True\n",
    ")\n",
    "\n",
    "print(\"Columnas tras tokenización:\", tokenized_datasets[\"train\"].column_names)\n",
    "print(f\"Ejemplos train: {len(tokenized_datasets['train'])}\")\n",
    "print(f\"Ejemplos test : {len(tokenized_datasets['test'])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1dcf27f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Ejemplo #1163574 ---\n",
      "PT original : Recolha e comparação de informação e criação de redes de especialistas e de institutos;\n",
      "PT tokenizado: Recolha e comparação de informação e criação de redes de especialistas e de institutos;\n",
      "ES referencia: Recopilación y cotejo de información y creación de redes de especialistas e institutos.\n",
      "ES tokenizado: Recopilación y cotejo de información y creación de redes de especialistas e institutos.\n",
      "\n",
      "--- Ejemplo #735697 ---\n",
      "PT original : Produz valor acrescentado em relação às actividades que visam promover a igualdade a nível nacional.\n",
      "PT tokenizado: Produz valor acrescentado em relação às actividades que visam promover a igualdade a nível nacional.\n",
      "ES referencia: Proporcionará un valor añadido a los trabajos en materia de igualdad realizados a escala nacional.\n",
      "ES tokenizado: Proporcionará un valor añadido a los trabajos en materia de igualdad realizados a escala nacional.\n",
      "\n",
      "--- Ejemplo #740577 ---\n",
      "PT original : Esta dotação destina-se a cobrir as despesas ligadas à promoção das relações entre o Parlamento Europeu e os parlamentos nacionais democraticamente eleitos de países terceiros, bem como as correspondentes organizações parlamentares regionais.\n",
      "PT tokenizado: Esta dotação destina-se a cobrir as despesas ligadas à promoção das relações entre o Parlamento Europeu e os parlamentos nacionais democraticamente eleitos de países terceiros, bem como as correspondentes organizações parlamentares regionais.\n",
      "ES referencia: Este crédito se destina a cubrir el gasto comprometido para promover las relaciones entre el Parlamento Europeo y los parlamentos nacionales democráticamente elegidos de terceros países, así como con las correspondientes organizaciones parlamentarias regionales.\n",
      "ES tokenizado: Este crédito se destina a cubrir el gasto comprometido para promover las relaciones entre el Parlamento Europeo y los parlamentos nacionales democráticamente elegidos de terceros países, así como con las correspondientes organizaciones parlamentarias regionales.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Revisamos ejemplos de tokenización\n",
    "from datasets import load_from_disk\n",
    "\n",
    "# 1) Recarga el dataset tokenizado y el raw split de entrenamiento\n",
    "tokenized = tokenized_datasets[\"train\"]\n",
    "raw_split = dataset_dict[\"train\"]   # contiene {\"pt\",\"es\"} originales\n",
    "\n",
    "# 2) Elige 3 índices dentro del rango de train\n",
    "for idx in random.sample(range(len(tokenized)), 3):\n",
    "    raw_item  = raw_split[idx]\n",
    "    tok_item  = tokenized[idx]\n",
    "    \n",
    "    pt_orig   = raw_item[\"pt\"]\n",
    "    es_ref    = raw_item[\"es\"]\n",
    "    # Decodifica solo los input_ids (entrada PT)\n",
    "    pt_tok    = tokenizer.decode(tok_item[\"input_ids\"], skip_special_tokens=True)\n",
    "    # Decodifica los labels (restaurando pad_token_id)\n",
    "    labels = [tid if tid != -100 else tokenizer.pad_token_id\n",
    "              for tid in tok_item[\"labels\"]]\n",
    "    es_tok = tokenizer.decode(labels, skip_special_tokens=True)\n",
    "    \n",
    "    print(f\"--- Ejemplo #{idx} ---\")\n",
    "    print(f\"PT original : {pt_orig}\")\n",
    "    print(f\"PT tokenizado: {pt_tok}\")\n",
    "    print(f\"ES referencia: {es_ref}\")\n",
    "    print(f\"ES tokenizado: {es_tok}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f3a8afd",
   "metadata": {},
   "source": [
    "Los ejemplos confirman que la tokenización:\n",
    "- Respeta la segmentación de sub-palabras en ambos idiomas sin introducir ni omitir caracteres: por ejemplo, el corchete y punto y coma en el tercer ejemplo aparecieron intactos.\n",
    "- Aplica truncamiento solo cuando sobrepasa 128 tokens, pero en estos casos las oraciones son más cortas, así que no hubo cortes.\n",
    "- Reproduce fielmente los labels, ya que al decodificar las labels (con padding convertido a –100) recuperamos exactamente la referencia en español sin tokens extra."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d8fd91ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b703e8dc6f5b45a08f25c64b912fd7cf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/7 shards):   0%|          | 0/1984064 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9f9cc50b344b42eaab8071effd256246",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/2 shards):   0%|          | 0/496017 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized datasets guardados en: ./data\\tokenized_facebook_m2m100_418M_len128\n"
     ]
    }
   ],
   "source": [
    "# Guardar el dataset tokenizado en disco\n",
    "# Construir un nombre de carpeta que incluya modelo y longitud máxima\n",
    "save_dir = os.path.join(\n",
    "    DATA_DIR,\n",
    "    f\"tokenized_{MODEL_NAME.replace('/', '_')}_len{MAX_LENGTH}\"\n",
    ")\n",
    "tokenized_datasets.save_to_disk(save_dir)\n",
    "print(f\"Tokenized datasets guardados en: {save_dir}\")\n",
    "\n",
    "# Recargar con:\n",
    "# from datasets import load_from_disk\n",
    "# tokenized_datasets = load_from_disk(save_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6c84ab9",
   "metadata": {},
   "source": [
    "Comentarios sobre la elección del tokenizer y parámetros\n",
    "\n",
    "- **Modelo M2M100 multilingüe**: se seleccinó `facebook/m2m100_418M` porque soporta de forma nativa la traducción directa PT→ES dentro de un único paso, evitando la complejidad de encadenar modelos. \n",
    "\n",
    "- **Configuración de idioma**: al establecer `tokenizer.src_lang = \"pt\"` y `tokenizer.tgt_lang = \"es\"`, se garantiza que el tokenizador segmente y represente los tokens correctamente según las características de cada idioma, mejorando la calidad de la generación.\n",
    "\n",
    "- **Uso de `text_target`**: evitamos el método obsoleto `as_target_tokenizer` pasando el texto de destino directamente con `text_target=batch[\"es\"]`. Esto unifica la tokenización de entrada y salida en una sola llamada, simplificando el flujo y eliminando warnings.\n",
    "\n",
    "- **Longitud máxima (`MAX_LENGTH=128`)**: equilibramos cobertura de contexto y eficiencia computacional. Un límite de 128 tokens cubre la gran mayoría de oraciones sin sobrepasar la memoria de la GPU, reduciendo la necesidad de truncamiento agresivo.\n",
    "\n",
    "- **Padding y `labels`**: aplicamos `padding=\"max_length\"` para batch uniformes y reemplazamos los `pad_token_id` en las etiquetas por `-100` para que no contribuyan a la función de pérdida, siguiendo la convención de Hugging Face para entrenamiento Seq2Seq.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
