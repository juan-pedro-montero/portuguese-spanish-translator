{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "08c52d94",
   "metadata": {},
   "source": [
    "# NOTEBOOK PARTE 2: EDA TRADUCTOR PT -> ES"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa342d0b",
   "metadata": {},
   "source": [
    "# CONFIGURACIÓN DEL ENTORNO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "111ff64c",
   "metadata": {},
   "source": [
    "## Librerías"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "529050a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from datasets import Dataset\n",
    "from transformers import AutoTokenizer\n",
    "import re\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import numpy as np\n",
    "import random\n",
    "from tqdm.auto import tqdm\n",
    "from transformers import set_seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e0fa0862",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\jpmon\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\jpmon\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download(\"punkt\")\n",
    "nltk.download(\"stopwords\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41fd8ced",
   "metadata": {},
   "source": [
    "## Reproducibilidad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "42111e82",
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 333\n",
    "set_seed(SEED)\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be799c3a",
   "metadata": {},
   "source": [
    "## Path de los datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "011bd31c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rutas configuradas:\n",
      "  PT→ ./data\\corpus_pt.txt\n",
      "  ES→ ./data\\corpus_es.txt\n"
     ]
    }
   ],
   "source": [
    "DATA_DIR = \"./data\"\n",
    "PT_FILE = os.path.join(DATA_DIR, \"corpus_pt.txt\")\n",
    "ES_FILE = os.path.join(DATA_DIR, \"corpus_es.txt\")\n",
    "\n",
    "assert os.path.isfile(PT_FILE), f\"No encontrado {PT_FILE}\"\n",
    "assert os.path.isfile(ES_FILE), f\"No encontrado {ES_FILE}\"\n",
    "\n",
    "print(f\"Rutas configuradas:\\n  PT→ {PT_FILE}\\n  ES→ {ES_FILE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f49e4815",
   "metadata": {},
   "source": [
    "# EDA & Limpieza de los datos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8554af7",
   "metadata": {},
   "source": [
    "## Carga de los datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1e934e39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pares totales originales: 6207844\n"
     ]
    }
   ],
   "source": [
    "# Rutas (definidas en el paso 0)\n",
    "pt_path = PT_FILE    # \"./data/corpus_pt.txt\"\n",
    "es_path = ES_FILE    # \"./data/corpus_es.txt\"\n",
    "\n",
    "# Leer líneas\n",
    "with open(pt_path, encoding=\"utf-8\") as f:\n",
    "    pt_lines = [line.strip() for line in f]\n",
    "with open(es_path, encoding=\"utf-8\") as f:\n",
    "    es_lines = [line.strip() for line in f]\n",
    "\n",
    "assert len(pt_lines) == len(es_lines), \"Los archivos PT y ES no tienen misma cantidad de líneas\"\n",
    "print(f\"Pares totales originales: {len(pt_lines)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68a86496",
   "metadata": {},
   "source": [
    "## Funciones de limpieza"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4d4d26ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "STOP_ES = set(stopwords.words(\"spanish\"))\n",
    "STOP_PT = set(stopwords.words(\"portuguese\"))\n",
    "\n",
    "def normalize_spaces(text):\n",
    "    \"\"\"Quita espacios y tabs múltiples, retornos de carro.\"\"\"\n",
    "    text = re.sub(r\"\\s+\", \" \", text)\n",
    "    return text.strip()\n",
    "\n",
    "def simple_tokenize(text):\n",
    "    \"\"\"Tokeniza con NLTK (útil para EDA de longitud).\"\"\"\n",
    "    return word_tokenize(text)\n",
    "\n",
    "def is_valid_pair(pt, es, \n",
    "                  min_len=2, max_len=256):\n",
    "    \"\"\"\n",
    "    Criterio de filtrado:\n",
    "      - No vacío\n",
    "      - Longitud en tokens entre min y max (inclusive)\n",
    "      - Texto PT y ES no idéntico\n",
    "    \"\"\"\n",
    "    if not pt or not es:\n",
    "        return False\n",
    "    t_pt = simple_tokenize(pt)\n",
    "    t_es = simple_tokenize(es)\n",
    "    if len(t_pt) < min_len or len(t_es) < min_len:\n",
    "        return False\n",
    "    if len(t_pt) > max_len or len(t_es) > max_len:\n",
    "        return False\n",
    "    if pt == es:\n",
    "        return False\n",
    "    return True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c242c933",
   "metadata": {},
   "source": [
    "## Estadísticas iniciales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "84544385",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8c00387e8cc64950ba42fee09df3f325",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Len PT rápido:   0%|          | 0/6207844 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "85dd8c59eb364c1eaaa41c7288f2baa2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Len ES rápido:   0%|          | 0/6207844 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Portugués (fast_len) ---\n",
      "Min   : 0\n",
      "10 pct: 1.0\n",
      "Median: 4.0\n",
      "90 pct: 39.0\n",
      "Max   : 9152\n",
      "\n",
      "--- Español (fast_len) ---\n",
      "Min   : 0\n",
      "10 pct: 1.0\n",
      "Median: 5.0\n",
      "90 pct: 42.0\n",
      "Max   : 9682\n",
      "\n",
      "Pares duplicados:      3459848\n",
      "Pares con vacíos:      16\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Estadísticas iniciales (rápido + tqdm)\n",
    "def fast_len(text: str) -> int:\n",
    "    \"\"\"Cuenta tokens con .split(), muy rápido.\"\"\"\n",
    "    return len(text.split())\n",
    "\n",
    "# Longitudes rápidas con tqdm\n",
    "tqdm.pandas()  # habilita progress_apply si quieres\n",
    "pt_lens = [fast_len(s) for s in tqdm(pt_lines, desc=\"Len PT rápido\")]\n",
    "es_lens = [fast_len(s) for s in tqdm(es_lines, desc=\"Len ES rápido\")]\n",
    "\n",
    "def print_stats(name: str, lengths: list):\n",
    "    print(f\"--- {name} ---\")\n",
    "    print(f\"Min   : {np.min(lengths)}\")\n",
    "    print(f\"10 pct: {np.percentile(lengths, 10)}\")\n",
    "    print(f\"Median: {np.median(lengths)}\")\n",
    "    print(f\"90 pct: {np.percentile(lengths, 90)}\")\n",
    "    print(f\"Max   : {np.max(lengths)}\")\n",
    "    print()\n",
    "\n",
    "print_stats(\"Portugués (fast_len)\", pt_lens)\n",
    "print_stats(\"Español (fast_len)\", es_lens)\n",
    "\n",
    "# Duplicados y vacíos (rápido, sin tqdm)\n",
    "pairs = list(zip(pt_lines, es_lines))\n",
    "num_dups  = len(pairs) - len(set(pairs))\n",
    "num_empty = sum(1 for p, e in pairs if not p or not e)\n",
    "\n",
    "print(f\"Pares duplicados:      {num_dups}\")\n",
    "print(f\"Pares con vacíos:      {num_empty}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "af0db988",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "abc0330a6ff74991b90cb35709d1c763",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Len PT NLTK sample:   0%|          | 0/200000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "35ada4a13c1f47e09d634ff86f9ce36a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Len ES NLTK sample:   0%|          | 0/200000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- PT (NLTK sample) ---\n",
      "Min   : 1\n",
      "10 pct: 1.0\n",
      "Median: 5.0\n",
      "90 pct: 43.0\n",
      "Max   : 7356\n",
      "\n",
      "--- ES (NLTK sample) ---\n",
      "Min   : 1\n",
      "10 pct: 1.0\n",
      "Median: 5.0\n",
      "90 pct: 46.0\n",
      "Max   : 1618\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Estadística “exacta” en submuestra con NLTK\n",
    "sample_n = 200_000\n",
    "idxs = random.sample(range(len(pt_lines)), sample_n)\n",
    "pt_sample = [pt_lines[i] for i in idxs]\n",
    "es_sample = [es_lines[i] for i in idxs]\n",
    "\n",
    "pt_lens_s = [len(simple_tokenize(s)) for s in tqdm(pt_sample, desc=\"Len PT NLTK sample\")]\n",
    "es_lens_s = [len(simple_tokenize(s)) for s in tqdm(es_sample, desc=\"Len ES NLTK sample\")]\n",
    "\n",
    "print_stats(\"PT (NLTK sample)\", pt_lens_s)\n",
    "print_stats(\"ES (NLTK sample)\", es_lens_s)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25925e3c",
   "metadata": {},
   "source": [
    "## Filtrado de ruido"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4bd68de2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8f3c907c86d94b729f58bbed28643c2d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filtrando pares:   0%|          | 0/6207844 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pares originales      : 6207844\n",
      "Pares tras filtrado   : 2480081\n",
      "Eliminados            : 3727763\n",
      "% eliminado           : 60.05%\n"
     ]
    }
   ],
   "source": [
    "# Crear set para eliminar duplicados y vacíos de forma eficiente\n",
    "unique_pairs = set()\n",
    "clean_pairs = []\n",
    "\n",
    "for pt, es in tqdm(zip(pt_lines, es_lines), total=len(pt_lines), desc=\"Filtrando pares\"):\n",
    "    # Normalizar espacios\n",
    "    pt_n = normalize_spaces(pt)\n",
    "    es_n = normalize_spaces(es)\n",
    "    # Salta vacíos\n",
    "    if not pt_n or not es_n:\n",
    "        continue\n",
    "    # Usa el tupla para detectar duplicados\n",
    "    pair = (pt_n, es_n)\n",
    "    if pair in unique_pairs:\n",
    "        continue\n",
    "    unique_pairs.add(pair)\n",
    "    # Aplica criterios de validación\n",
    "    if is_valid_pair(pt_n, es_n, min_len=2, max_len=256):\n",
    "        clean_pairs.append(pair)\n",
    "\n",
    "# Se descomprime en dos listas PT y ES\n",
    "pt_clean, es_clean = zip(*clean_pairs)\n",
    "\n",
    "# Reporte\n",
    "total_original = len(pt_lines)\n",
    "total_filtered = len(clean_pairs)\n",
    "num_removed = total_original - total_filtered\n",
    "\n",
    "print(f\"Pares originales      : {total_original}\")\n",
    "print(f\"Pares tras filtrado   : {total_filtered}\")\n",
    "print(f\"Eliminados            : {num_removed}\")\n",
    "print(f\"% eliminado           : {num_removed/total_original*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "469a3bb7",
   "metadata": {},
   "source": [
    "## Construir Dataset final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6a58c510",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "87b602ef536b415b93bfa890d4f2eb02",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating json from Arrow format:   0%|          | 0/2481 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corpus limpio guardado en:\n",
      "   • ./data\\clean_corpus_pt.txt\n",
      "   • ./data\\clean_corpus_es.txt\n",
      "   • ./data\\clean_corpus_pt_es.json\n"
     ]
    }
   ],
   "source": [
    "# Guardar archivos de texto\n",
    "with open(os.path.join(DATA_DIR, \"clean_corpus_pt.txt\"), \"w\", encoding=\"utf-8\") as f_pt, \\\n",
    "     open(os.path.join(DATA_DIR, \"clean_corpus_es.txt\"), \"w\", encoding=\"utf-8\") as f_es:\n",
    "    for pt, es in clean_pairs:\n",
    "        f_pt.write(pt + \"\\n\")\n",
    "        f_es.write(es + \"\\n\")\n",
    "\n",
    "# Guardar como Dataset JSON (opción Hugging Face)\n",
    "clean_ds = Dataset.from_dict({\"pt\": list(pt_clean), \"es\": list(es_clean)})\n",
    "clean_ds.to_json(os.path.join(DATA_DIR, \"clean_corpus_pt_es.json\"))\n",
    "\n",
    "print(\"Corpus limpio guardado en:\")\n",
    "print(\"   •\", os.path.join(DATA_DIR, \"clean_corpus_pt.txt\"))\n",
    "print(\"   •\", os.path.join(DATA_DIR, \"clean_corpus_es.txt\"))\n",
    "print(\"   •\", os.path.join(DATA_DIR, \"clean_corpus_pt_es.json\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e48e62d3",
   "metadata": {},
   "source": [
    "# Comentarios "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4530448e",
   "metadata": {},
   "source": [
    "- Calidad y cobertura del corpus:\n",
    "Tras extraer y alinear las de 6 millones de oraciones del Parlamento Europeo, se comprobó una amplia cobertura temática y un rango muy variado de longitudes (de 1 a casi 10,000 tokens). Esto confconfirmó que se cuenta con un recurso suficientemente grande, aunque heterogéneo, para entrenar un traductor PT→ES robusto.\n",
    "\n",
    "- Reducción de ruido y duplicados:\n",
    "El filtrado inicial eliminó más del 60% de los pares, principalmente por duplicados exactos y longitudes extremas. Aunque parezca un recorte drástico, quedarnos con ~2.5 M de pares de calidad media/alta facilitará el entrenamiento y evitará que el modelo “memorice” fragmentos repetidos o aprenda de ejemplos demasiado largos o irrelevantes.\n",
    "\n",
    "- Elección de umbrales y criterios de limpieza:\n",
    "Definimos como válidas las oraciones con entre 2 y 256 tokens, y descartamos pares idénticos. Estos límites balancean la necesidad de capturar construcciones complejas contra la eficiencia de entrenamiento y la memoria. En futuros refinamientos podríamos ajustar estos umbrales o añadir chequeos como la relación de longitud PT/ES o la eliminación de oraciones con demasiados números o símbolos.\n",
    "\n",
    "- Modularidad y reproducibilidad del flujo:\n",
    "Se decidió dividir el pipeline en dos notebooks —uno para limpieza y otro para modelado— para obtener flexibilidad: con esto podemos iterar en los criterios de EDA sin reentrenar, y lanzar rápidamente nuevos experimentos de fine-tuning usando el corpus limpio ya almacenado. Además, los archivos planos (.txt) garantizan compatibilidad con cualquier script o framework.\n",
    "\n",
    "- Próximos pasos:\n",
    "Con el conjunto limpio listo, el siguiente paso será la división en train/test y la tokenización específica para el modelo. Será importante mantener la semilla y la misma lógica de particionado para asegurar que las comparaciones de configuraciones de entrenamiento sean consistentes y reproducibles."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
