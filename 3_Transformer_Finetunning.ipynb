{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "08c52d94",
   "metadata": {},
   "source": [
    "# NOTEBOOK PARTE 2: TRANSFORMER FINETUNING TRADUCTOR PT -> ES"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa342d0b",
   "metadata": {},
   "source": [
    "# CONFIGURACIÓN DEL ENTORNO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "111ff64c",
   "metadata": {},
   "source": [
    "## Librerías"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "529050a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\jpmon\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"2\"   # Filtra mensajes INFO y WARNING de TF\n",
    "import random\n",
    "import torch\n",
    "from tqdm.auto import tqdm\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from datasets import load_from_disk, DatasetDict\n",
    "from transformers import (\n",
    "    set_seed,\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSeq2SeqLM,\n",
    "    Seq2SeqTrainingArguments,\n",
    "    Seq2SeqTrainer\n",
    ")\n",
    "from peft import get_peft_config, get_peft_model, LoraConfig\n",
    "import sacrebleu"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41fd8ced",
   "metadata": {},
   "source": [
    "## Reproducibilidad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "42111e82",
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 333\n",
    "set_seed(SEED)\n",
    "random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f408fc24",
   "metadata": {},
   "source": [
    "## Selección dispositivo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ca4eac2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dispositivo: cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Dispositivo:\", device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be799c3a",
   "metadata": {},
   "source": [
    "## Path del corpus tokenizado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4e32d822",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized data directory: ./data\\tokenized_facebook_m2m100_418M_len128\n"
     ]
    }
   ],
   "source": [
    "DATA_DIR      = \"./data\"\n",
    "TOKENIZED_DIR = os.path.join(DATA_DIR, f\"tokenized_facebook_m2m100_418M_len128\")\n",
    "assert os.path.isdir(TOKENIZED_DIR), f\"No existe el directorio tokenizado: {TOKENIZED_DIR}\"\n",
    "print(\"Tokenized data directory:\", TOKENIZED_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6c41296",
   "metadata": {},
   "source": [
    "# Carga del corpus tokenizado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f5279676",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Splits disponibles: ['train', 'test']\n",
      "Tamaño train original: 1984064\n",
      "Tamaño test            : 496017\n",
      "Tamaño train reducido : 40000\n",
      "Tamaño test reducido  : 10000\n"
     ]
    }
   ],
   "source": [
    "# Cargar tokenizer\n",
    "MODEL_NAME = \"facebook/m2m100_418M\"\n",
    "tokenizer  = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "tokenizer.src_lang = \"pt\"\n",
    "tokenizer.tgt_lang = \"es\"\n",
    "\n",
    "tokenized_datasets = load_from_disk(TOKENIZED_DIR)\n",
    "\n",
    "# — Verificaciones antes de muestreo —\n",
    "print(\"Splits disponibles:\", list(tokenized_datasets.keys()))\n",
    "print(f\"Tamaño train original: {len(tokenized_datasets['train'])}\")\n",
    "print(f\"Tamaño test            : {len(tokenized_datasets['test'])}\")\n",
    "\n",
    "# — Muestreo para reducir el tiempo de entrenamiento\n",
    "train_full   = tokenized_datasets[\"train\"]\n",
    "train_sample = train_full.shuffle(seed=SEED).select(range(40_000))\n",
    "tokenized_datasets[\"train\"] = train_sample\n",
    "\n",
    "test_full    = tokenized_datasets[\"test\"]\n",
    "test_sample  = test_full.shuffle(seed=SEED).select(range(10_000))\n",
    "tokenized_datasets[\"test\"]  = test_sample\n",
    "\n",
    "print(f\"Tamaño train reducido : {len(tokenized_datasets['train'])}\")\n",
    "print(f\"Tamaño test reducido  : {len(tokenized_datasets['test'])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2591f458",
   "metadata": {},
   "source": [
    "# Selección y configuración del modelo pre-entrenado"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4e4086e",
   "metadata": {},
   "source": [
    "Como modelo pre entrenado se seleccionó facebook/m2m100_418M, un sistema Multilingual Machine Translation de Facebook que soporta de forma nativa la traducción directa de portugués a español. Su arquitectura Seq2Seq basada en Transformers, pre-entrenada sobre cientos de pares de idiomas, aporta un conocimiento lingüístico muy sólido desde el arranque, lo que se traduce en mejores resultados y convergencia más rápida, desarrollandose en un solo paso. Para adaptar eficientemente este “foundation model” al dominio específico del Parlamento Europeo, se empleó PEFT (Parameter-Efficient Fine-Tuning), esta es una metodología que inyecta un pequeño conjunto de parámetros adicionales (para este caso LoRA) sin tener que actualizar todos los millones de pesos originales. Con esto, se busca reducir drásticamente el uso de memoria y el tiempo de entrenamiento, intentando mantener la precisión del modelo completo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a3a86081",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "M2M100ForConditionalGeneration(\n",
       "  (model): M2M100Model(\n",
       "    (shared): M2M100ScaledWordEmbedding(128112, 1024, padding_idx=1)\n",
       "    (encoder): M2M100Encoder(\n",
       "      (embed_tokens): M2M100ScaledWordEmbedding(128112, 1024, padding_idx=1)\n",
       "      (embed_positions): M2M100SinusoidalPositionalEmbedding()\n",
       "      (layers): ModuleList(\n",
       "        (0-11): 12 x M2M100EncoderLayer(\n",
       "          (self_attn): M2M100SdpaAttention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (activation_fn): ReLU()\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "      (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (decoder): M2M100Decoder(\n",
       "      (embed_tokens): M2M100ScaledWordEmbedding(128112, 1024, padding_idx=1)\n",
       "      (embed_positions): M2M100SinusoidalPositionalEmbedding()\n",
       "      (layers): ModuleList(\n",
       "        (0-11): 12 x M2M100DecoderLayer(\n",
       "          (self_attn): M2M100SdpaAttention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (activation_fn): ReLU()\n",
       "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (encoder_attn): M2M100SdpaAttention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "      (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "  )\n",
       "  (lm_head): Linear(in_features=1024, out_features=128112, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cargar modelo\n",
    "# MODEL_NAME = \"facebook/m2m100_418M\" # ya cargado\n",
    "base_model = AutoModelForSeq2SeqLM.from_pretrained(MODEL_NAME)\n",
    "base_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "40fe356d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuración de LoRA (PEFT)\n",
    "lora_config = LoraConfig(\n",
    "    r=8,                       # rango bajo para la factorización de la matriz\n",
    "    lora_alpha=32,             # escala de aprendizaje relativa\n",
    "    target_modules=[\"q_proj\", \"v_proj\"],  # módulos a adaptar\n",
    "    lora_dropout=0.05,         # dropout para regularización\n",
    "    bias=\"none\",               # no ajustar bias\n",
    "    task_type=\"SEQ_2_SEQ_LM\"   # tarea Seq2Seq\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9ab08ac0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 1,179,648 || all params: 485,085,184 || trainable%: 0.2432\n"
     ]
    }
   ],
   "source": [
    "# Envolver el modelo con PEFT-LoRA\n",
    "model = get_peft_model(base_model, lora_config)\n",
    "model.print_trainable_parameters()  # para verificar cuántos parámetros se afinan"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dbc0ef3",
   "metadata": {},
   "source": [
    "# Definición de Hiperparámetros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ee1abfb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seq2SeqTrainingArguments(\n",
      "_n_gpu=1,\n",
      "accelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None, 'use_configured_state': False},\n",
      "adafactor=False,\n",
      "adam_beta1=0.9,\n",
      "adam_beta2=0.999,\n",
      "adam_epsilon=1e-08,\n",
      "auto_find_batch_size=False,\n",
      "average_tokens_across_devices=False,\n",
      "batch_eval_metrics=False,\n",
      "bf16=False,\n",
      "bf16_full_eval=False,\n",
      "data_seed=None,\n",
      "dataloader_drop_last=False,\n",
      "dataloader_num_workers=0,\n",
      "dataloader_persistent_workers=False,\n",
      "dataloader_pin_memory=True,\n",
      "dataloader_prefetch_factor=None,\n",
      "ddp_backend=None,\n",
      "ddp_broadcast_buffers=None,\n",
      "ddp_bucket_cap_mb=None,\n",
      "ddp_find_unused_parameters=None,\n",
      "ddp_timeout=1800,\n",
      "debug=[],\n",
      "deepspeed=None,\n",
      "disable_tqdm=False,\n",
      "do_eval=True,\n",
      "do_predict=False,\n",
      "do_train=False,\n",
      "eval_accumulation_steps=None,\n",
      "eval_delay=0,\n",
      "eval_do_concat_batches=True,\n",
      "eval_on_start=False,\n",
      "eval_steps=0,\n",
      "eval_strategy=no,\n",
      "eval_use_gather_object=False,\n",
      "fp16=True,\n",
      "fp16_backend=auto,\n",
      "fp16_full_eval=False,\n",
      "fp16_opt_level=O1,\n",
      "fsdp=[],\n",
      "fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},\n",
      "fsdp_min_num_params=0,\n",
      "fsdp_transformer_layer_cls_to_wrap=None,\n",
      "full_determinism=False,\n",
      "generation_config=None,\n",
      "generation_max_length=None,\n",
      "generation_num_beams=None,\n",
      "gradient_accumulation_steps=1,\n",
      "gradient_checkpointing=False,\n",
      "gradient_checkpointing_kwargs=None,\n",
      "greater_is_better=None,\n",
      "group_by_length=False,\n",
      "half_precision_backend=auto,\n",
      "hub_always_push=False,\n",
      "hub_model_id=None,\n",
      "hub_private_repo=None,\n",
      "hub_strategy=every_save,\n",
      "hub_token=<HUB_TOKEN>,\n",
      "ignore_data_skip=False,\n",
      "include_for_metrics=[],\n",
      "include_inputs_for_metrics=False,\n",
      "include_num_input_tokens_seen=False,\n",
      "include_tokens_per_second=False,\n",
      "jit_mode_eval=False,\n",
      "label_names=None,\n",
      "label_smoothing_factor=0.0,\n",
      "learning_rate=5e-05,\n",
      "length_column_name=length,\n",
      "load_best_model_at_end=False,\n",
      "local_rank=0,\n",
      "log_level=passive,\n",
      "log_level_replica=warning,\n",
      "log_on_each_node=True,\n",
      "logging_dir=./outputs/m2m100_finetuned_lora\\runs\\Jun18_15-27-26_RZ-G16-JP,\n",
      "logging_first_step=False,\n",
      "logging_nan_inf_filter=True,\n",
      "logging_steps=500,\n",
      "logging_strategy=steps,\n",
      "lr_scheduler_kwargs={},\n",
      "lr_scheduler_type=linear,\n",
      "max_grad_norm=1.0,\n",
      "max_steps=-1,\n",
      "metric_for_best_model=None,\n",
      "mp_parameters=,\n",
      "neftune_noise_alpha=None,\n",
      "no_cuda=False,\n",
      "num_train_epochs=2,\n",
      "optim=adamw_torch,\n",
      "optim_args=None,\n",
      "optim_target_modules=None,\n",
      "output_dir=./outputs/m2m100_finetuned_lora,\n",
      "overwrite_output_dir=False,\n",
      "past_index=-1,\n",
      "per_device_eval_batch_size=32,\n",
      "per_device_train_batch_size=16,\n",
      "predict_with_generate=True,\n",
      "prediction_loss_only=False,\n",
      "push_to_hub=False,\n",
      "push_to_hub_model_id=None,\n",
      "push_to_hub_organization=None,\n",
      "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
      "ray_scope=last,\n",
      "remove_unused_columns=True,\n",
      "report_to=['tensorboard'],\n",
      "restore_callback_states_from_checkpoint=False,\n",
      "resume_from_checkpoint=None,\n",
      "run_name=./outputs/m2m100_finetuned_lora,\n",
      "save_on_each_node=False,\n",
      "save_only_model=False,\n",
      "save_safetensors=True,\n",
      "save_steps=0,\n",
      "save_strategy=steps,\n",
      "save_total_limit=2,\n",
      "seed=42,\n",
      "skip_memory_metrics=True,\n",
      "sortish_sampler=False,\n",
      "tf32=None,\n",
      "torch_compile=False,\n",
      "torch_compile_backend=None,\n",
      "torch_compile_mode=None,\n",
      "torch_empty_cache_steps=None,\n",
      "torchdynamo=None,\n",
      "tpu_metrics_debug=False,\n",
      "tpu_num_cores=None,\n",
      "use_cpu=False,\n",
      "use_ipex=False,\n",
      "use_legacy_prediction_loop=False,\n",
      "use_liger_kernel=False,\n",
      "use_mps_device=False,\n",
      "warmup_ratio=0.1,\n",
      "warmup_steps=0,\n",
      "weight_decay=0.0,\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Directorio de salida para checkpoints y logs\n",
    "OUTPUT_DIR = \"./outputs/m2m100_finetuned_lora\"\n",
    "\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=OUTPUT_DIR,\n",
    "    per_device_train_batch_size=16,     # subimos a 16\n",
    "    per_device_eval_batch_size=32,      # evalúa más rápido\n",
    "    gradient_accumulation_steps=1,      # sin acumulación\n",
    "\n",
    "    do_eval=True,\n",
    "    eval_steps=0,   # evalúa al final de cada época\n",
    "    save_steps=0,   # guarda al final de cada época\n",
    "    save_total_limit=2,\n",
    "\n",
    "    logging_steps=500,\n",
    "    num_train_epochs=2,                 # entrenar 1 épocas\n",
    "    learning_rate=5e-5,\n",
    "    warmup_ratio=0.1,\n",
    "\n",
    "    predict_with_generate=True,        # desactivar durante training\n",
    "    fp16=True\n",
    ")\n",
    "\n",
    "print(training_args)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57c78e4c",
   "metadata": {},
   "source": [
    "# Métricas de Evaluación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3e873e57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definir la función de métricas (BLEU y Perplexity)\n",
    "def compute_metrics(eval_preds):\n",
    "    preds, labels = eval_preds\n",
    "    # Decodificar las predicciones\n",
    "    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
    "    # Decodificar las etiquetas\n",
    "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "    # BLEU\n",
    "    bleu = sacrebleu.corpus_bleu(decoded_preds, [decoded_labels]).score\n",
    "    # Perplexity (usando la pérdida retornada por trainer.evaluate())\n",
    "    # Trainer ya computa 'eval_loss', así que simples:\n",
    "    perplexity = np.exp(eval_preds.metrics[\"eval_loss\"])\n",
    "    return {\"bleu\": bleu, \"perplexity\": perplexity}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f247cee2",
   "metadata": {},
   "source": [
    "# Entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "37c875d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jpmon\\AppData\\Local\\Temp\\ipykernel_18108\\524095280.py:3: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Seq2SeqTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Seq2SeqTrainer(\n",
      "No label_names provided for model class `PeftModelForSeq2SeqLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n",
      "Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.58.0. You should pass an instance of `EncoderDecoderCache` instead, e.g. `past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='5000' max='5000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [5000/5000 59:52, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>1.493900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>1.392400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>1.373000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>1.345400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>1.391000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>1.352000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>1.336600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>1.340500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4500</td>\n",
       "      <td>1.337900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>1.344000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=5000, training_loss=1.3706663208007812, metrics={'train_runtime': 3594.4227, 'train_samples_per_second': 22.257, 'train_steps_per_second': 1.391, 'total_flos': 2.174352359424e+16, 'train_loss': 1.3706663208007812, 'epoch': 2.0})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Instanciar el Trainer y lanzar el entrenamiento\n",
    "# Crear el Trainer\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"test\"],         # tu subset de validación reducido\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "# Lanzar entrenamiento\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74763d7c",
   "metadata": {},
   "source": [
    "## Guardar el modelo para luego ejecutar en modo inferencia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1b885f44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modelo y tokenizer guardados en: ./outputs/m2m100_finetuned_lora_inferencia\n"
     ]
    }
   ],
   "source": [
    "# Nombre del directorio de inferencia\n",
    "INFER_DIR = \"./outputs/m2m100_finetuned_lora_inferencia\"\n",
    "\n",
    "# Crear carpeta si no existe\n",
    "os.makedirs(INFER_DIR, exist_ok=True)\n",
    "\n",
    "# Guardar modelo y tokenizer\n",
    "model.save_pretrained(INFER_DIR)\n",
    "tokenizer.save_pretrained(INFER_DIR)\n",
    "\n",
    "print(f\"Modelo y tokenizer guardados en: {INFER_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd08dc81",
   "metadata": {},
   "source": [
    "# Análisis de Métricas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "62577b7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluando sólo 1000 ejemplos para métricas rápidas.\n"
     ]
    }
   ],
   "source": [
    "# Crear subset de evaluación ligero —\n",
    "# Asumimos que `tokenized_datasets` ya está cargado\n",
    "# y que tu split \"test\" está reducido o completo.\n",
    "SAMPLE_SIZE = 1000\n",
    "light_test = tokenized_datasets[\"test\"].shuffle(seed=SEED).select(range(SAMPLE_SIZE))\n",
    "print(f\"Evaluando sólo {len(light_test)} ejemplos para métricas rápidas.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7232016b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "813d5614ca4f4fca9b2e8e936039e995",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subconjunto raw_test para generación: 1000 ejemplos\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a93326b900f34a0fbab9fff05a7f4c12",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'pred': 'ARTÍCULO 1, PONTO 9-B (nuevo)'}\n",
      "{'pred': '-//EP//TEXT REPORT A6-2007-0494 0 NOT XML V0//PT'}\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "#Carga del raw test (JSON HuggingFace)\n",
    "raw_test = load_dataset(\n",
    "    \"json\",\n",
    "    data_files={\"test\": \"./data/clean_corpus_pt_es.json\"},\n",
    "    field=None,\n",
    "    split=\"test\"\n",
    ")\n",
    "\n",
    "# Subset ligero para métricas\n",
    "SAMPLE_SIZE = 1000\n",
    "light_raw = raw_test.shuffle(seed=SEED).select(range(SAMPLE_SIZE))\n",
    "print(f\"Subconjunto raw_test para generación: {len(light_raw)} ejemplos\")\n",
    "\n",
    "# Preparar modelo/tokenizer para inferencia\n",
    "INFER_DIR = \"./outputs/m2m100_finetuned_lora_infer\"\n",
    "device    = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "tokenizer_inf = AutoTokenizer.from_pretrained(\n",
    "    INFER_DIR,\n",
    "    local_files_only=True\n",
    ")\n",
    "model_inf = AutoModelForSeq2SeqLM.from_pretrained(\n",
    "    INFER_DIR,\n",
    "    local_files_only=True\n",
    ").to(device)\n",
    "\n",
    "tokenizer_inf.src_lang = \"pt\"\n",
    "tokenizer_inf.tgt_lang = \"es\"\n",
    "forced_bos = tokenizer_inf.get_lang_id(\"es\")\n",
    "\n",
    "# Función de generación por batch (greedy)\n",
    "def generate_batch(batch):\n",
    "    # batch[\"pt\"] y batch[\"es\"] están disponibles en light_raw\n",
    "    inputs = tokenizer_inf(\n",
    "        batch[\"pt\"],\n",
    "        return_tensors=\"pt\",\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        max_length=128\n",
    "    )\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "    out = model_inf.generate(\n",
    "        **inputs,\n",
    "        forced_bos_token_id=forced_bos,\n",
    "        num_beams=1,       # greedy decode\n",
    "        max_new_tokens=60\n",
    "    )\n",
    "    decoded = tokenizer_inf.batch_decode(out, skip_special_tokens=True)\n",
    "    return {\"pred\": decoded}\n",
    "\n",
    "# Ejecutar generación sobre el subset raw\n",
    "light_preds = light_raw.map(\n",
    "    generate_batch,\n",
    "    batched=True,\n",
    "    batch_size=64,\n",
    "    remove_columns=light_raw.column_names\n",
    ")\n",
    "\n",
    "# Ya tienes light_preds[\"pred\"] con las traducciones\n",
    "print(light_preds[0])\n",
    "print(light_preds[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "4744f8ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU en subset (1000 ej): 34.56\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jpmon\\AppData\\Local\\Temp\\ipykernel_18108\\711444320.py:14: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Seq2SeqTrainer.__init__`. Use `processing_class` instead.\n",
      "  ghost_trainer = Seq2SeqTrainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexity en subset: 3.46\n"
     ]
    }
   ],
   "source": [
    "# Extraer referencias e hipótesis\n",
    "refs = light_raw[\"es\"]          # lista de cadenas en español\n",
    "hyps = light_preds[\"pred\"]      # lista de traducciones generadas\n",
    "\n",
    "# Calcular BLEU\n",
    "bleu = sacrebleu.corpus_bleu(hyps, [refs]).score\n",
    "print(f\"BLEU en subset ({len(refs)} ej): {bleu:.2f}\")\n",
    "\n",
    "# Preparar tokenized subset para evaluar pérdida\n",
    "#    (usamos el mismo submuestreo pero tokenizado)\n",
    "light_tok = tokenized_datasets[\"test\"].shuffle(seed=SEED).select(range(len(refs)))\n",
    "\n",
    "# Ghost trainer para pérdida\n",
    "ghost_trainer = Seq2SeqTrainer(\n",
    "    model=model_inf,\n",
    "    args=training_args,           # los mismos args de fine-tuning\n",
    "    tokenizer=tokenizer_inf,\n",
    "    eval_dataset=light_tok\n",
    ")\n",
    "pred_out = ghost_trainer.predict(light_tok)\n",
    "\n",
    "# Extraer loss y calcular Perplexity\n",
    "#    Fíjate en la clave exacta de metrics: puede ser 'test_loss' o 'eval_loss'\n",
    "loss_key = \"test_loss\" if \"test_loss\" in pred_out.metrics else \"eval_loss\"\n",
    "eval_loss = pred_out.metrics[loss_key]\n",
    "perplexity = np.exp(eval_loss)\n",
    "print(f\"Perplexity en subset: {perplexity:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "500e1551",
   "metadata": {},
   "source": [
    "# Comentarios y conclusiones"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "979c32bc",
   "metadata": {},
   "source": [
    "**Rendimiento cuantitativo sólido:** Con un BLEU de 34.56 en el subset de 1,000 ejemplos, el modelo demuestra una capacidad de traducción competitiva para PT→ES, capturando con buena fidelidad léxico y estructura. Un BLEU superior a 30 suele considerarse de alta calidad en tareas de traducción automática de pares de idiomas con divergencias sintácticas.\n",
    "\n",
    "**Perplexity baja indica fluidez:** La perplexity resultante de 3.46 sugiere que el modelo asigna probabilidades elevadas a las secuencias de referencia en español. Valores cercanos a 1 indicarían predicciones casi perfectas; aunque 3.46 no es ideal, es muy razonable para un fine-tuning de dos épocas sobre datos limitados, y apunta a que la generación es coherente y fluida.\n",
    "\n",
    "**Trade-off tiempo vs precisión:** Al usar un subset de evaluación de solo 1,000 ejemplos con decoding greedy, se ha acelerado el cálculo de métricas a 30s para BLEU y 1 min para perplexity. Esto valida que la estrategia de muestreo ligero ofreció una estimación fiable sin requerir horas de cómputo.\n",
    "\n",
    "En conjunto, el fine-tuning con LoRA sobre M2M100 ha logrado un traductor PT→ES eficaz y eficiente, con métricas sólidas y un flujo de trabajo adaptable al tiempo y recursos disponibles."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
